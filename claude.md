# AI Judge 문제 검증 시스템

## 개요

수학 문제 생성 시 AI-as-a-Judge 패턴을 사용하여 생성된 문제의 품질을 자동 검증합니다.

## 아키텍처

```
문제 생성 요청
    ↓
generate_problems() (최대 3회 재시도)
    ↓
Gemini-2.5-pro로 문제 생성 (문제 생성 AI)
    ↓
각 문제마다 _validate_with_ai_judge() 호출
    ↓
OpenAI GPT-4o-mini로 검증 (검증 AI)
    ↓
VALID → 통과
INVALID → 피드백 수집 → 재생성
```

### 왜 이중 AI 시스템인가?

1. **생성 AI (Gemini-2.5-pro)**: 고품질 문제 생성에 특화
2. **검증 AI (OpenAI GPT-4o-mini)**: 객관적 품질 평가에 특화
   - 안전 필터 문제 해결
   - 빠른 검증 속도
   - 낮은 비용

## 검증 기준 (LaTeX 문법 제외)

### 1. mathematical_accuracy (수학적 정확성)
- 문제 자체에 논리적 모순이 없는가?
- 예: "x > 5이고 x < 3인 x를 구하시오" → 불합격

### 2. consistency (정답 일치성) ⭐ 가장 중요
- 해설을 통해 도출한 답 = `correct_answer`
- 객관식: `correct_answer`가 A,B,C,D 중 하나이고, `choices[해당인덱스]`가 실제 정답과 일치
- 단답형: 해설의 최종 답 = `correct_answer`

### 3. completeness (완결성)
- 객관식: `choices` 정확히 4개 존재, 중복 없음
- 해설: 문제 풀이에 필요한 단계가 모두 포함됨

### 4. logic_flow (해설의 논리성)
- 해설이 문제를 풀 수 있는 순차적 단계를 제공하는가?

## 통과 기준

- **consistency >= 4점** (80% 이상)
- 나머지 항목 평균 >= 3.5점 (70% 이상)

## 구현 위치

**파일**: `backend/services/math-service/app/services/problem_generator.py`

### 주요 메서드

#### `_validate_with_ai_judge(problem: Dict) -> tuple`
- **OpenAI GPT-4o-mini 사용** (Gemini 안전 필터 문제 해결)
- temperature=0.1 (일관성)
- max_tokens=500
- JSON 모드 강제 (`response_format={"type": "json_object"}`)
- 영어 프롬프트로 빠른 처리
- Returns: `(is_valid: bool, scores: dict, feedback: str)`

#### `_rebuild_prompt_with_feedback(original_prompt: str, invalid_problems: List[Dict]) -> str`
- 검증 실패 시 피드백을 프롬프트에 추가
- 재생성 시 동일한 오류 반복 방지

#### `_call_ai_and_parse_response(prompt: str, max_retries: int = 3, target_count: int = None) -> List[Dict]`
- 문제 생성 후 자동으로 검증 실행
- **부분 재생성 로직**: 합격한 문제는 유지, 불합격만 재생성
- 최대 3회 재시도
- 모든 시도 실패 시 예외 발생

#### `_adjust_prompt_for_needed_count(original_prompt: str, needed_count: int) -> str`
- 부족한 문제 개수만큼만 생성하도록 프롬프트 조정
- 정규표현식으로 문제 개수 패턴 교체

## 비용 최적화

1. **OpenAI GPT-4o-mini 사용**: 검증용으로 빠르고 저렴한 모델
   - Gemini-2.5-flash 대비 안전 필터 문제 없음
   - 검증 1회당 약 $0.0001 (매우 저렴)
2. **영어 프롬프트**: 토큰 절약 및 처리 속도 향상
3. **부분 재생성**: 합격한 문제는 재생성하지 않음
4. **검증 실패 시에만 재시도**: 불필요한 API 호출 최소화

## 로깅

검증 과정은 콘솔에 출력됩니다:

```
============================================================
문제 생성 시도 1/3
현재 합격: 0개 / 목표: 10개
추가 필요: 10개
============================================================

🔍 AI Judge 검증 시작 - 10개 문제
  ✅ 문제 1번: VALID - 평균 4.2점 [수학정확성:4.0 정답일치:5.0 완결성:4.0 논리성:4.0]
  ✅ 문제 2번: VALID - 평균 4.5점 [수학정확성:5.0 정답일치:5.0 완결성:4.0 논리성:4.0]
  ❌ 문제 3번: INVALID - 평균 3.5점 [수학정확성:4.0 정답일치:3.0 완결성:3.5 논리성:3.5]
     💬 피드백: Explanation's final answer doesn't match correct_answer

⚠️ 부족: 1개 추가 생성 필요 (현재 9/10)
```

## 작동 흐름

1. Celery 비동기 태스크에서 `generate_math_problems_task` 실행
2. `math_service._generate_problems_with_ratio()` 호출
3. `problem_generator.generate_problems()` 실행
4. 각 문제 생성 후 자동 검증
5. **합격 문제는 누적 보관, 불합격만 재생성**
6. 목표 개수 달성 시 DB 저장
7. 최대 3회 재시도 (부족한 개수만큼만)

## 검증 예외 처리

**네트워크 오류 (TimeoutError, ConnectionError, OSError):**
- 기본 통과 처리 (너무 엄격하지 않게)
- 기본 점수 4.0 부여
- 로그: "⚠️ 네트워크 오류로 검증 생략, 기본 통과 처리"

**JSON 파싱 오류 (JSONDecodeError):**
- 예외 재발생 → 재시도 유도
- 로그: "❌ AI Judge 응답 JSON 파싱 실패"

**기타 오류:**
- 예외 재발생 → 재시도 유도
- 로그: "❌ AI Judge 검증 오류"

## 주의사항

- **LaTeX 문법은 검증하지 않음**: 프론트엔드에서 렌더링 처리
- **논리적 정합성만 검증**: 수식 오류, 정답 불일치 등
- **재시도 제한**: 3회 시도 후에도 실패 시 예외 발생

## 성능

- 검증 1회당 약 0.5~1초 소요 (gemini-2.5-flash)
- 10개 문제 생성 + 검증: 약 15~20초
- 재시도 1회 추가 시: +15~20초

## 개선 완료 사항 (2025-09-30)

✅ **OpenAI로 AI Judge 교체**: Gemini 안전 필터 문제 완전 해결
✅ **예외 처리 엄격화**: 네트워크 오류만 통과, 나머지는 재시도
✅ **검증 기준 완화**: consistency >= 4점, 전체 평균 >= 3.5점 (이전: consistency = 5점 필수)
✅ **부분 재생성 로직**: 합격한 문제는 유지, 불합격만 재생성
✅ **검증 로깅 개선**: 상세 점수 및 피드백 출력

## 향후 개선 사항

1. 검증 결과 DB 저장 (통계/분석용)
2. 배치 검증 (10개 문제를 한 번에 검증)
3. 검증 기준 동적 조정 (난이도별 다른 기준)
4. 수학적 동치성 검증 (sympy 활용)

---

## 📢 외부 설명용 (투자자/사용자/개발자에게)

### Q. 수학 문제 검증을 어떻게 하나요?

**A. AI-as-a-Judge 패턴을 사용한 이중 AI 검증 시스템**

```
1️⃣ 문제 생성: Google Gemini-2.5-pro
   - 세계 최고 수준의 수학 문제 생성 AI
   - 한국 교육과정(쎈 교재) 기반 프롬프트

2️⃣ 품질 검증: OpenAI GPT-4o-mini
   - 독립적인 AI가 객관적으로 평가
   - 4가지 기준으로 엄격하게 심사
```

### Q. 검증 기준이 뭔가요?

**A. 4가지 평가 기준 (각 1~5점 척도)**

| 기준 | 설명 | 비중 |
|------|------|------|
| **mathematical_accuracy** | 수학적 오류가 없는가? | 일반 |
| **consistency** ⭐ | 해설의 답 = 정답? | **가장 중요** |
| **completeness** | 필수 정보 모두 있는가? | 일반 |
| **logic_flow** | 해설이 논리적인가? | 일반 |

### Q. 합격 기준이 뭔가요?

**A. 엄격한 이중 조건**

```
✅ consistency >= 4점 (80% 이상) 필수
AND
✅ 나머지 평균 >= 3.5점 (70% 이상)
```

- **불합격 시**: 피드백과 함께 재생성 (최대 3회)
- **3회 실패 시**: 예외 발생, 문제 생성 중단

### Q. 평균 점수가 4.5~5점인데, 너무 관대한 거 아닌가요?

**A. 아닙니다! 오히려 Gemini-2.5-pro의 우수성을 증명합니다.**

1. **높은 점수 = 좋은 문제**
   - 평균 4.5점: 거의 완벽한 문제
   - 평균 5점: 완벽한 문제 (수학적 오류 없음, 정답 일치)

2. **실제 검증 통계** (예시)
   ```
   10개 문제 생성 요청
   → 1차 시도: 9개 합격, 1개 불합격 (90% 성공률)
   → 2차 시도: 1개 재생성 → 합격
   → 최종: 10개 모두 평균 4.5점 이상
   ```

3. **불합격 사례**
   ```
   ❌ 문제 3번: INVALID - 평균 3.2점
      [수학정확성:4.0 정답일치:2.0 완결성:3.5 논리성:3.5]
      💬 피드백: Explanation's final answer (25) doesn't match correct_answer (24)
   ```

### Q. 왜 이중 AI 시스템을 쓰나요?

**A. 객관성과 신뢰성 확보**

| 역할 | AI 모델 | 이유 |
|------|---------|------|
| 생성 | Gemini-2.5-pro | 수학 문제 생성에 특화 |
| 검증 | OpenAI GPT-4o-mini | 독립적 평가 + 안전 필터 없음 |

- **생성 AI가 자기 검증?** → 편향 가능성
- **독립된 AI가 검증** → 객관적 평가 보장

### Q. 비용은 얼마나 드나요?

**A. 문제당 약 $0.01 이하 (매우 저렴)**

```
문제 1개당 비용:
- 생성 (Gemini-2.5-pro): ~$0.008
- 검증 (GPT-4o-mini): ~$0.0001
- 총: ~$0.0081 (약 10원)

문제 100개 생성: 약 $0.81 (1,000원)
```

### Q. 검증 속도는?

**A. 문제당 평균 1초 (매우 빠름)**

```
- 문제 1개 검증: 0.5~1초
- 문제 10개 동시 검증: 약 5~10초
- 재시도 포함 전체: 15~30초
```

---

## 🎯 핵심 메시지

> **"AI가 만든 문제를 다른 AI가 검증하는 이중 품질 관리 시스템"**
>
> - ✅ 객관적 평가 (독립된 AI)
> - ✅ 엄격한 기준 (consistency 80% 필수)
> - ✅ 자동 재생성 (불합격 시)
> - ✅ 투명한 점수 (4가지 지표 공개)
> - ✅ 높은 품질 (평균 4.5점 이상)