# ìˆ˜í•™ ë¬¸ì œ ìƒì„± ì‹œìŠ¤í…œ í”Œë¡œìš° (Math Problem Generation Flow)

**ë²„ì „**: v1.0
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-10-02
**ì‹œìŠ¤í…œ**: math-service

## ëª©ì°¨
1. [ì‹œìŠ¤í…œ ê°œìš”](#ì‹œìŠ¤í…œ-ê°œìš”)
2. [ì „ì²´ ë¬¸ì œ ìƒì„± í”Œë¡œìš°](#ì „ì²´-ë¬¸ì œ-ìƒì„±-í”Œë¡œìš°)
3. [AI Judge ê²€ì¦ ì‹œìŠ¤í…œ](#ai-judge-ê²€ì¦-ì‹œìŠ¤í…œ)
4. [ë‹¨ì¼ ë¬¸ì œ ì¬ìƒì„± í”Œë¡œìš°](#ë‹¨ì¼-ë¬¸ì œ-ì¬ìƒì„±-í”Œë¡œìš°)
5. [TikZ ê·¸ë˜í”„ ìƒì„±](#tikz-ê·¸ë˜í”„-ìƒì„±)
6. [ëª¨ë“ˆ êµ¬ì¡°](#ëª¨ë“ˆ-êµ¬ì¡°)
7. [ë²„ì „ íˆìŠ¤í† ë¦¬](#ë²„ì „-íˆìŠ¤í† ë¦¬)

---

## ì‹œìŠ¤í…œ ê°œìš”

ìˆ˜í•™ ë¬¸ì œ ìƒì„± ì‹œìŠ¤í…œì€ **Gemini 2.5 Pro**ì™€ **GPT-4o-mini**ë¥¼ í™œìš©í•œ ì´ì¤‘ ê²€ì¦ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### í•µì‹¬ íŠ¹ì§•
- **ì´ì¤‘ AI ëª¨ë¸**: Gemini 2.5 Pro(ìƒì„±) + GPT-4o-mini(ê²€ì¦)
- **AI Judge ê²€ì¦**: ëª¨ë“  ë¬¸ì œëŠ” 4ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€ (ìˆ˜í•™ì •í™•ì„±, ì •ë‹µì¼ì¹˜, ì™„ê²°ì„±, ë…¼ë¦¬ì„±)
- **ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜**: ë¶ˆí•©ê²© ë¬¸ì œì— ëŒ€í•œ í”¼ë“œë°± ê¸°ë°˜ ì¬ìƒì„± (ìµœëŒ€ 3íšŒ)
- **ë¶€ë¶„ ì¬ìƒì„±**: ì¼ë¶€ ë¬¸ì œë§Œ ë¶ˆí•©ê²©í•´ë„ ë¶€ì¡±í•œ ê°œìˆ˜ë§Œ ì¬ìƒì„±
- **ë³‘ë ¬ ì²˜ë¦¬**: ThreadPoolExecutor í™œìš©í•œ ê³ ì† ìƒì„±
- **TikZ ì§€ì›**: ê·¸ë˜í”„ ë‹¨ì›ì—ì„œ ìë™ ì‹œê°í™” ìƒì„±
- **3ë‹¨ê³„ ë‚œì´ë„**: A(ì§ì ‘ê³„ì‚°), B(ì‘ìš©ë²ˆì—­), C(í†µí•©ë°œê²¬)

### ì‚¬ìš© ëª¨ë¸
- **ë¬¸ì œ ìƒì„±**: Gemini 2.5 Pro (`gemini-2.0-flash-exp`)
- **AI Judge ê²€ì¦**: GPT-4o-mini (OpenAI)

---

## ì „ì²´ ë¬¸ì œ ìƒì„± í”Œë¡œìš°

### 1ë‹¨ê³„: ì‚¬ìš©ì ìš”ì²­ ì ‘ìˆ˜

```
POST /api/worksheets/generate
{
  "school_level": "ì¤‘í•™êµ",
  "grade": 1,
  "semester": "1í•™ê¸°",
  "unit_name": "ì†Œì¸ìˆ˜ë¶„í•´",
  "chapter_name": "ì†Œì¸ìˆ˜ë¶„í•´ì˜ í™œìš©",
  "problem_count": 5,
  "difficulty_ratio": {"A": 2, "B": 2, "C": 1},
  "problem_type_ratio": {"multiple_choice": 3, "short_answer": 2},
  "user_prompt": "ì‹¤ìƒí™œ ë¬¸ì œ í¬í•¨"
}
```

### 2ë‹¨ê³„: Celery ë¹„ë™ê¸° íƒœìŠ¤í¬ ì‹¤í–‰

```python
# tasks.py:generate_math_problems_task
task = generate_math_problems_task.delay(request, user_id)
â†’ task_id ë°˜í™˜ (ì˜ˆ: "abc123-def456-...")
```

### 3ë‹¨ê³„: êµìœ¡ê³¼ì • ë°ì´í„° ë¡œë“œ

```python
# math_generation_service.py
curriculum_data = self._load_curriculum_data(
    school_level, grade, semester, unit_name, chapter_name
)

# middle1_math_curriculum.jsonì—ì„œ ë¡œë“œ
{
  "school_level": "ì¤‘í•™êµ",
  "grade": 1,
  "semester": "1í•™ê¸°",
  "chapters": [
    {
      "unit_name": "ì†Œì¸ìˆ˜ë¶„í•´",
      "chapter_name": "ì†Œì¸ìˆ˜ë¶„í•´ì˜ í™œìš©",
      "description": "...",
      "core_concepts": ["ìµœëŒ€ê³µì•½ìˆ˜", "ìµœì†Œê³µë°°ìˆ˜"]
    }
  ]
}
```

### 4ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ë¬¸ì œ ìƒì„± ìš”ì²­

```python
# prompt_templates.py
prompt = PromptTemplates.build_problem_generation_prompt(
    curriculum_data=curriculum_data,
    user_prompt=user_prompt,
    problem_count=5,
    difficulty_distribution="Aë‹¨ê³„ 2ê°œ, Bë‹¨ê³„ 2ê°œ, Cë‹¨ê³„ 1ê°œ"
)

# í•µì‹¬ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°:
"""
You are a Master Test Creator for "SSEN" textbook.

**#1. CORE MISSION**
- Topic: ì¤‘1 1í•™ê¸° - ì†Œì¸ìˆ˜ë¶„í•´ > ì†Œì¸ìˆ˜ë¶„í•´ì˜ í™œìš©
- User Request: "ì‹¤ìƒí™œ ë¬¸ì œ í¬í•¨"
- Total Problems: 5
- Required Distribution: Aë‹¨ê³„ 2ê°œ, Bë‹¨ê³„ 2ê°œ, Cë‹¨ê³„ 1ê°œ

**#2. MENTAL SANDBOX FOR EACH DIFFICULTY LEVEL**

### A-LEVEL SANDBOX: Direct Computation
- Test if student memorized formula
- 1-2 computational steps
- 30ì´ˆ ë‚´ í•´ê²° ê°€ëŠ¥
- STRICTLY FORBIDDEN: Word problems

### B-LEVEL SANDBOX: Application & Translation
- Translate situation â†’ equation â†’ solve
- 3-4 steps
- Word problems, real-life scenarios
- STRICTLY FORBIDDEN: Direct computation or pattern discovery

### C-LEVEL SANDBOX: Synthesis & Discovery (HARDEST)
- Synthesize multiple concepts
- 5+ steps with "aha!" moment
- Optimization, pattern discovery, proof
- STRICTLY FORBIDDEN: Just harder B-Level problems
"""
```

### 5ë‹¨ê³„: Gemini API í˜¸ì¶œ ë° ì‘ë‹µ íŒŒì‹±

```python
# problem_generator.py:_call_ai_and_parse_response()

valid_problems = []
max_retries = 3

for retry_attempt in range(max_retries):
    needed_count = target_count - len(valid_problems)

    # ë¶€ì¡±í•œ ê°œìˆ˜ë§Œí¼ í”„ë¡¬í”„íŠ¸ ì¡°ì •
    if len(valid_problems) > 0:
        adjusted_prompt = self._adjust_prompt_for_needed_count(
            original_prompt, needed_count
        )

    # Gemini API í˜¸ì¶œ
    response = self.model.generate_content(adjusted_prompt)
    problems = self._extract_and_parse_json(response.text)

    # AI Judge ê²€ì¦ (ë‹¤ìŒ ë‹¨ê³„)
    ...
```

### 6ë‹¨ê³„: AI Judge ê²€ì¦ (GPT-4o-mini)

```python
# problem_generator.py:_validate_with_ai_judge()

for idx, problem in enumerate(problems):
    is_valid, scores, feedback = self._validate_with_ai_judge(problem)

    if is_valid:
        valid_problems.append(problem)
        print(f"âœ… ë¬¸ì œ {len(valid_problems)}ë²ˆ: VALID - í‰ê·  {scores['overall_score']:.1f}ì ")
    else:
        print(f"âŒ ë¬¸ì œ {idx+1}ë²ˆ: INVALID - í‰ê·  {scores['overall_score']:.1f}ì ")
        print(f"ğŸ’¬ í”¼ë“œë°±: {feedback}")
        invalid_problems.append({
            "problem": problem,
            "feedback": feedback,
            "scores": scores
        })
```

**AI Judge ê²€ì¦ ê¸°ì¤€** (4ê°€ì§€ í•­ëª©, ê° 1-5ì ):

```python
validation_prompt = """
Evaluation criteria:
1. mathematical_accuracy (1-5): No mathematical or logical errors
2. consistency (1-5): Explanation's answer matches correct_answer
3. completeness (1-5): All required fields present (ê°ê´€ì‹ì€ 4ê°œ ë³´ê¸°)
4. logic_flow (1-5): Explanation is logical and easy to follow

Decision rule:
- consistency â‰¥ 4.0 (í•„ìˆ˜)
- AND average of other scores â‰¥ 3.5
â†’ "VALID"
"""
```

**ê²€ì¦ ìƒì„¸ ì¶œë ¥**:
```
âœ… ë¬¸ì œ 1ë²ˆ: VALID - í‰ê·  4.5ì  [ìˆ˜í•™ì •í™•ì„±:5.0 ì •ë‹µì¼ì¹˜:5.0 ì™„ê²°ì„±:4.0 ë…¼ë¦¬ì„±:4.0]
âŒ ë¬¸ì œ 2ë²ˆ: INVALID - í‰ê·  3.2ì  [ìˆ˜í•™ì •í™•ì„±:4.0 ì •ë‹µì¼ì¹˜:2.5 ì™„ê²°ì„±:3.5 ë…¼ë¦¬ì„±:3.0]
   ğŸ’¬ í”¼ë“œë°±: í’€ì´ ê³¼ì •ì˜ ë§ˆì§€ë§‰ ë‹µì´ ì •ë‹µê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
```

### 7ë‹¨ê³„: ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ (Feedback-Enhanced Regeneration)

```python
# ë¶ˆí•©ê²© ë¬¸ì œê°€ ìˆê³  ì•„ì§ ì¬ì‹œë„ ê°€ëŠ¥í•œ ê²½ìš°
if len(valid_problems) < target_count and retry_attempt < max_retries - 1:
    shortage = target_count - len(valid_problems)
    print(f"âš ï¸ ë¶€ì¡±: {shortage}ê°œ ì¶”ê°€ ìƒì„± í•„ìš” (í˜„ì¬ {len(valid_problems)}/{target_count})")

    # í”¼ë“œë°±ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±
    if invalid_problems:
        prompt = self._rebuild_prompt_with_feedback(original_prompt, invalid_problems)
```

**í”¼ë“œë°± ê°•í™” í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ**:
```
[ì›ë³¸ í”„ë¡¬í”„íŠ¸]

**IMPORTANT: Previous attempt had issues. Fix these:**

Problem 1 feedback:
- Scores: mathematical_accuracy=4.0, consistency=2.5, completeness=3.5, logic_flow=3.0
- Issue: í’€ì´ ê³¼ì •ì˜ ë§ˆì§€ë§‰ ë‹µì´ ì •ë‹µê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤

**MUST ensure**: consistency >= 4 (explanation's answer = correct_answer), all scores >= 3.5
```

### 8ë‹¨ê³„: ë¬¸ì œ ì €ì¥ ë° ì›Œí¬ì‹œíŠ¸ ìƒì„±

```python
# tasks.py

# Worksheet ìƒì„±
worksheet = Worksheet(
    title=f"{chapter_name} ë¬¸ì œ {problem_count}ê°œ",
    teacher_id=user_id,
    school_level=school_level,
    grade=grade,
    semester=semester,
    unit_name=unit_name,
    chapter_name=chapter_name,
    problem_count=problem_count,
    difficulty_ratio=difficulty_ratio,
    problem_type_ratio=problem_type_ratio,
    status=WorksheetStatus.COMPLETED
)
db.add(worksheet)
db.flush()

# Problem ì €ì¥ (TikZ ì§€ì›)
for idx, problem_data in enumerate(problems):
    problem = Problem(
        worksheet_id=worksheet.id,
        sequence_order=idx + 1,
        problem_type=problem_data['problem_type'],
        difficulty=problem_data['difficulty'],
        question=problem_data['question'],
        choices=json.dumps(problem_data.get('choices')),
        correct_answer=problem_data['correct_answer'],
        explanation=problem_data['explanation'],
        has_diagram=str(problem_data.get('has_diagram', False)).lower(),
        diagram_type=problem_data.get('diagram_type'),
        tikz_code=problem_data.get('tikz_code')  # TikZ LaTeX ì½”ë“œ
    )
    db.add(problem)

db.commit()
```

---

## AI Judge ê²€ì¦ ì‹œìŠ¤í…œ

### ê²€ì¦ ì ˆì°¨

```mermaid
graph TD
    A[ìƒì„±ëœ ë¬¸ì œ] --> B{AI Judge<br/>GPT-4o-mini}
    B --> C[4ê°€ì§€ ê¸°ì¤€ í‰ê°€]
    C --> D{consistency â‰¥ 4.0<br/>AND<br/>others avg â‰¥ 3.5?}
    D -->|Yes| E[âœ… VALID<br/>valid_problemsì— ì¶”ê°€]
    D -->|No| F[âŒ INVALID<br/>invalid_problemsì— ì¶”ê°€<br/>+ í”¼ë“œë°± ì €ì¥]
    E --> G{ëª©í‘œ ë‹¬ì„±?}
    F --> G
    G -->|Yes| H[ì™„ë£Œ]
    G -->|No, ì¬ì‹œë„ ê°€ëŠ¥| I[í”¼ë“œë°± í¬í•¨ ì¬ìƒì„±]
    G -->|No, ì¬ì‹œë„ ì†Œì§„| J[âŒ ì‹¤íŒ¨]
    I --> A
```

### ê²€ì¦ ê¸°ì¤€ ìƒì„¸

| í•­ëª© | ì„¤ëª… | ì ìˆ˜ ë²”ìœ„ |
|------|------|----------|
| **mathematical_accuracy** | ìˆ˜í•™ì  ì˜¤ë¥˜ ì—†ìŒ | 1-5 |
| **consistency** | í’€ì´ì˜ ìµœì¢… ë‹µ = correct_answer | 1-5 |
| **completeness** | í•„ìˆ˜ í•„ë“œ ì™„ë¹„ (ê°ê´€ì‹ì€ 4ê°œ ë³´ê¸°) | 1-5 |
| **logic_flow** | í’€ì´ ë…¼ë¦¬ì„± ë° ì´í•´ ìš©ì´ì„± | 1-5 |

**í•©ê²© ì¡°ê±´**:
1. **consistency â‰¥ 4.0** (í•„ìˆ˜, ê°€ì¥ ì¤‘ìš”)
2. **AND** (mathematical_accuracy + completeness + logic_flow) / 3 â‰¥ 3.5

### ì½”ë“œ ì˜ˆì‹œ

```python
# problem_generator.py:717-796

def _validate_with_ai_judge(self, problem: Dict) -> tuple:
    """
    AI Judgeë¡œ ë¬¸ì œ ê²€ì¦ (OpenAI GPT-4o-mini)
    Returns: (is_valid: bool, scores: dict, feedback: str)
    """

    validation_prompt = f"""You are a math education expert. Please validate the following math problem.

The problem data is as follows:
- Question: {question}
- Correct Answer: {correct_answer}
- Explanation: {explanation}
- Problem Type: {problem_type}
- Choices: {choices_text}

Evaluation criteria:
1. mathematical_accuracy (1-5): No mathematical or logical errors.
2. consistency (1-5): The final answer in the explanation matches the correct_answer.
3. completeness (1-5): All required fields are present (e.g., multiple_choice must have 4 choices).
4. logic_flow (1-5): The explanation is logical and easy to follow.

Return ONLY valid JSON (no markdown, no code blocks):
{{
  "scores": {{"mathematical_accuracy": <score>, "consistency": <score>, "completeness": <score>, "logic_flow": <score>}},
  "overall_score": <average>,
  "decision": "VALID" or "INVALID",
  "feedback": "<brief feedback>"
}}

Decision rule: `consistency` must be 4 or higher, AND the average of the other scores must be 3.5 or higher to be "VALID".
"""

    response = self.openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a math education expert who validates math problems and returns structured JSON responses."},
            {"role": "user", "content": validation_prompt}
        ],
        temperature=0.1,
        max_tokens=500,
        response_format={"type": "json_object"}
    )

    result = json.loads(response.choices[0].message.content.strip())

    is_valid = result.get('decision') == 'VALID'
    scores = result.get('scores', {})
    scores['overall_score'] = result.get('overall_score', 0)
    feedback = result.get('feedback', 'No feedback')

    return is_valid, scores, feedback
```

---

## ë‹¨ì¼ ë¬¸ì œ ì¬ìƒì„± í”Œë¡œìš°

### ì‚¬ìš©ì ìš”ì²­

```
POST /api/problems/regenerate-async
{
  "problem_id": 123,
  "requirements": "ë” ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
  "current_problem": {
    "question": "...",
    "correct_answer": "...",
    "explanation": "..."
  }
}
```

### Celery íƒœìŠ¤í¬ ì‹¤í–‰

```python
# tasks.py:194-240 regenerate_single_problem_task

task = regenerate_single_problem_task.delay(
    problem_id=123,
    requirements="ë” ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
    current_problem={...}
)

# 1. ë¬¸ì œ ì •ë³´ ì¡°íšŒ
problem = db.query(Problem).filter(Problem.id == problem_id).first()
worksheet = db.query(Worksheet).filter(Worksheet.id == problem.worksheet_id).first()

# 2. AI ì¬ìƒì„± ìš”ì²­
ai_service = AIService()
new_problem_data = ai_service.regenerate_single_problem(
    current_problem=current_problem,
    requirements=requirements
)

# 3. ë¬¸ì œ ì—…ë°ì´íŠ¸
problem.question = new_problem_data.get("question")
problem.correct_answer = new_problem_data.get("correct_answer")
problem.explanation = new_problem_data.get("explanation")
if new_problem_data.get("choices"):
    problem.choices = json.dumps(new_problem_data["choices"], ensure_ascii=False)

# TikZ ì§€ì›
if "tikz_code" in new_problem_data:
    tikz_code = new_problem_data.get("tikz_code")
    problem.tikz_code = tikz_code if tikz_code else None
    if tikz_code:
        problem.has_diagram = 'true'

db.commit()
```

### ì¬ìƒì„± í”„ë¡¬í”„íŠ¸ êµ¬ì¡°

```python
# ai_service.py (ì¶”ì •)

regeneration_prompt = f"""
You are a math problem expert. Regenerate the following problem based on the user's requirements.

**Current Problem**:
- Question: {current_problem['question']}
- Correct Answer: {current_problem['correct_answer']}
- Explanation: {current_problem['explanation']}

**User Requirements**: {requirements}

Please regenerate the problem following the same structure but adjusted according to the requirements.
Return JSON with the same fields: question, correct_answer, explanation, choices (if applicable).
"""
```

---

## TikZ ê·¸ë˜í”„ ìƒì„±

### ê·¸ë˜í”„ ë‹¨ì› íŠ¹ë³„ ì²˜ë¦¬

**ì ìš© ë‹¨ì›**: "ê·¸ë˜í”„ì™€ ë¹„ë¡€" (ì¢Œí‘œí‰ë©´, ì •ë¹„ë¡€, ë°˜ë¹„ë¡€)

```python
# prompt_templates.py:20-94

is_graph_unit = curriculum_data.get('unit_name') == "ê·¸ë˜í”„ì™€ ë¹„ë¡€"

if is_graph_unit:
    graph_instruction = """
**SPECIAL INSTRUCTION FOR ê·¸ë˜í”„ì™€ ë¹„ë¡€ (Graph and Proportion Unit)**:
- MUST include graph visualizations using TikZ LaTeX for at least 60% of problems
- Use "has_diagram": true, "diagram_type": "coordinate_plane" or "function_graph"
- Include "tikz_code": "[Full TikZ LaTeX code]"
"""
```

### TikZ ì½”ë“œ ì˜ˆì‹œ

```json
{
  "question": "ë‹¤ìŒ ê·¸ë˜í”„ëŠ” $y = 2x$ì˜ ê·¸ë˜í”„ì´ë‹¤. ì  $A$ì˜ ì¢Œí‘œë¥¼ êµ¬í•˜ì—¬ë¼.",
  "choices": ["$(1, 2)$", "$(2, 4)$", "$(3, 6)$", "$(4, 8)$"],
  "correct_answer": "B",
  "explanation": "ì •ë¹„ë¡€ ê´€ê³„ $y = 2x$ì—ì„œ $x = 2$ì¼ ë•Œ $y = 4$ì´ë¯€ë¡œ ì  $A$ì˜ ì¢Œí‘œëŠ” $(2, 4)$ì´ë‹¤.",
  "problem_type": "multiple_choice",
  "difficulty": "A",
  "has_diagram": true,
  "diagram_type": "function_graph",
  "tikz_code": "\\begin{tikzpicture}[scale=0.8]\n  \\draw[->] (-1,0) -- (5,0) node[right] {$x$};\n  \\draw[->] (0,-1) -- (0,5) node[above] {$y$};\n  \\draw[thick,blue] (0,0) -- (4,4) node[midway,above left] {$y=2x$};\n  \\filldraw[red] (2,4) circle (2pt) node[above right] {$A$};\n  \\foreach \\x in {1,2,3,4}\n    \\draw (\\x,0.1) -- (\\x,-0.1) node[below] {$\\x$};\n  \\foreach \\y in {1,2,3,4}\n    \\draw (0.1,\\y) -- (-0.1,\\y) node[left] {$\\y$};\n\\end{tikzpicture}"
}
```

### í”„ë¡ íŠ¸ì—”ë“œ ë Œë”ë§

í”„ë¡ íŠ¸ì—”ë“œì—ì„œëŠ” `tikz_code`ë¥¼ ë°›ì•„ì„œ LaTeX ë Œë”ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‹œê°í™”:

```typescript
// TikZRenderer.tsx (ê¸°ì¡´ íŒŒì¼)
// tikz_codeë¥¼ ë°›ì•„ì„œ SVGë¡œ ë Œë”ë§
```

### ë‹µì•ˆ ì€ë‹‰ ê·œì¹™

**ì¤‘ìš”**: ë¬¸ì œì—ì„œ ë¬»ëŠ” ì ì€ ê·¸ë˜í”„ì— í‘œì‹œí•˜ì§€ ì•ŠìŒ

```python
# prompt_templates.py:85-91

"""
**ANSWER POINT HIDING RULE (ë§¤ìš° ì¤‘ìš”)**:
- If the question asks to find a specific point's coordinate (e.g., "ì  Dì˜ ì¢Œí‘œë¥¼ êµ¬í•˜ì‹œì˜¤"),
  that point is the ANSWER
- **DO NOT draw or label the answer point on the graph**
- Only show the GIVEN points (ì£¼ì–´ì§„ ì ) on the graph
- Example: If question asks "Find point D" and gives "A(1,2), B(5,2), C(6,5)",
  only draw points A, B, C
- **NEVER use \\coordinate (D) at (x,y) or \\filldraw for the answer point**
"""
```

---

## ëª¨ë“ˆ êµ¬ì¡°

### íŒŒì¼ êµ¬ì„±

```
math-service/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ problem_generator.py          # 832ì¤„ (ë©”ì¸ ìƒì„± ë¡œì§)
â”‚   â”‚   â”œâ”€â”€ math_generation_service.py    # êµìœ¡ê³¼ì • ë¡œë“œ, ì›Œí¬ì‹œíŠ¸ ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ prompt_templates.py           # 209ì¤„ (í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿)
â”‚   â”‚   â””â”€â”€ ai_service.py                 # ë‹¨ì¼ ë¬¸ì œ ì¬ìƒì„±
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ worksheet.py                  # ì›Œí¬ì‹œíŠ¸ CRUD ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚   â””â”€â”€ problem.py                    # ë¬¸ì œ ìˆ˜ì •, ì¬ìƒì„± ì—”ë“œí¬ì¸íŠ¸
â”‚   â”œâ”€â”€ tasks.py                          # 609ì¤„ (Celery íƒœìŠ¤í¬)
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ problem.py                    # Problem ëª¨ë¸ (TikZ í•„ë“œ í¬í•¨)
â”‚       â””â”€â”€ worksheet.py                  # Worksheet ëª¨ë¸
â””â”€â”€ data/
    â””â”€â”€ middle1_math_curriculum.json      # ì¤‘1 êµìœ¡ê³¼ì • ë°ì´í„°
```

### í•µì‹¬ í´ë˜ìŠ¤

#### ProblemGenerator (problem_generator.py)

```python
class ProblemGenerator:
    """ë©”ì¸ ë¬¸ì œ ìƒì„± í´ë˜ìŠ¤"""

    def __init__(self):
        self.model = genai.GenerativeModel("gemini-2.0-flash-exp")
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def generate_problems_parallel(self, ...):
        """ë³‘ë ¬ ë¬¸ì œ ìƒì„± (ThreadPoolExecutor)"""

    def _generate_single_problem(self, ...):
        """ë‹¨ì¼ ë¬¸ì œ ìƒì„± (ì¬ì‹œë„ í¬í•¨)"""

    def _call_ai_and_parse_response(self, prompt, max_retries=3, target_count=None):
        """AI í˜¸ì¶œ ë° ì‘ë‹µ íŒŒì‹± - ë¶€ë¶„ ì¬ìƒì„± ë¡œì§ í¬í•¨"""

    def _validate_with_ai_judge(self, problem):
        """AI Judgeë¡œ ë¬¸ì œ ê²€ì¦ (OpenAI GPT-4o-mini)"""

    def _rebuild_prompt_with_feedback(self, original_prompt, invalid_problems):
        """í”¼ë“œë°±ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±"""
```

#### MathGenerationService (math_generation_service.py)

```python
class MathGenerationService:
    """ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜"""

    def _load_curriculum_data(self, ...):
        """êµìœ¡ê³¼ì • ë°ì´í„° ë¡œë“œ (middle1_math_curriculum.json)"""

    def _generate_problems_with_ratio(self, ...):
        """ë‚œì´ë„/ìœ í˜• ë¹„ìœ¨ì— ë”°ë¥¸ ë¬¸ì œ ìƒì„±"""

    @staticmethod
    def copy_worksheet(db, source_worksheet_id, target_user_id, new_title):
        """ì›Œí¬ì‹œíŠ¸ ë³µì‚¬ (ë§ˆì¼“ êµ¬ë§¤ìš©)"""
```

---

## ë²„ì „ íˆìŠ¤í† ë¦¬

### v1.0 (2025-10-02)
- ì´ˆê¸° ë¬¸ì„œí™” ì‘ì„±
- Gemini 2.5 Pro + GPT-4o-mini ì´ì¤‘ ê²€ì¦ ì‹œìŠ¤í…œ ì„¤ëª…
- AI Judge ê²€ì¦ ê¸°ì¤€ (4ê°€ì§€ í•­ëª©) ìƒì„¸í™”
- ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ (í”¼ë“œë°± ê°•í™”) ë¬¸ì„œí™”
- ë¶€ë¶„ ì¬ìƒì„± ë¡œì§ ì„¤ëª… ì¶”ê°€
- TikZ ê·¸ë˜í”„ ìƒì„± í”Œë¡œìš° ì¶”ê°€
- ë‹¨ì¼ ë¬¸ì œ ì¬ìƒì„± í”Œë¡œìš° ì¶”ê°€
- ëª¨ë“ˆ êµ¬ì¡° ì •ë¦¬ (832ì¤„ ë©”ì¸ íŒŒì¼)
