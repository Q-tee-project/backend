"""
ìˆ˜í•™ ë¬¸ì œ ìƒì„± ë¡œì§ ë¶„ë¦¬ - JSON íŒŒì‹± ì™„ë²½ ê°œì„ 
"""
import os
import json
import re
import google.generativeai as genai
from openai import OpenAI
from typing import Dict, List, Any, Optional
from .prompt_templates import PromptTemplates
from dotenv import load_dotenv

load_dotenv()

class ProblemGenerator:
    """ìˆ˜í•™ ë¬¸ì œ ìƒì„± ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self):
        # AI ëª¨ë¸ ì§ì ‘ ì´ˆê¸°í™” (ìˆœí™˜ import ë°©ì§€)
        gemini_api_key = os.getenv("GEMINI_API_KEY")
        if not gemini_api_key:
            raise ValueError("GEMINI_API_KEY environment variable is required")

        genai.configure(api_key=gemini_api_key)

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (AI Judgeìš©)
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is required")

        self.openai_client = OpenAI(api_key=openai_api_key)

        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë³µì› (íƒ€ì„ì•„ì›ƒ í•´ê²°ì„ ìœ„í•´ í† í° ìˆ˜ ì¡°ì •)
        generation_config = genai.types.GenerationConfig(
            temperature=0.7,
            max_output_tokens=12288,
            top_p=0.8,
            top_k=40
        )

        self.model = genai.GenerativeModel(
            'gemini-2.5-pro',
            generation_config=generation_config
        )

        self.prompt_templates = PromptTemplates()
    
    def generate_problems(
        self,
        curriculum_data: Dict,
        user_prompt: str,
        problem_count: int = 1,
        difficulty_ratio: Dict = None,
        problem_type: str = None
    ) -> List[Dict]:
        """ìˆ˜í•™ ë¬¸ì œ ìƒì„± ë©”ì¸ ë¡œì§"""
        
        # ë‚œì´ë„ ë¶„ë°° ê³„ì‚°
        difficulty_distribution = self._calculate_difficulty_distribution(
            problem_count, difficulty_ratio
        )

        # problem_typeì´ ì§€ì •ëœ ê²½ìš° ê°•ì œ ì œì•½ ì¶”ê°€
        enhanced_user_prompt = user_prompt
        if problem_type:
            if problem_type == "multiple_choice":
                type_constraint = f"""

**ì ˆëŒ€ ì¤€ìˆ˜ ì‚¬í•­ - ìœ„ë°˜ ì‹œ ì‹¤íŒ¨:**
- ì •í™•íˆ {problem_count}ê°œì˜ ê°ê´€ì‹ ë¬¸ì œë§Œ ìƒì„±
- ëª¨ë“  ë¬¸ì œì˜ problem_typeì€ "multiple_choice"
- ê° ë¬¸ì œëŠ” ì •ë‹µì´ 1ê°œë§Œ ì¡´ì¬ (A, B, C, D ì¤‘ í•˜ë‚˜)
- choicesëŠ” ì •í™•íˆ 4ê°œ
- ë‹¨ë‹µí˜•ì´ë‚˜ ì„œìˆ í˜• ë¬¸ì œëŠ” ì ˆëŒ€ ìƒì„± ê¸ˆì§€
"""
            elif problem_type == "short_answer":
                type_constraint = f"""

**ì ˆëŒ€ ì¤€ìˆ˜ ì‚¬í•­ - ìœ„ë°˜ ì‹œ ì‹¤íŒ¨:**
- ì •í™•íˆ {problem_count}ê°œì˜ ë‹¨ë‹µí˜• ë¬¸ì œë§Œ ìƒì„±
- ëª¨ë“  ë¬¸ì œì˜ problem_typeì€ "short_answer"
- choices í•„ë“œëŠ” null ë˜ëŠ” ë¹ˆ ë°°ì—´
- ê°ê´€ì‹ì´ë‚˜ ì„œìˆ í˜• ë¬¸ì œëŠ” ì ˆëŒ€ ìƒì„± ê¸ˆì§€
"""
            else:
                type_constraint = ""

            enhanced_user_prompt = f"{user_prompt}{type_constraint}"

        # í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
        prompt = self.prompt_templates.build_problem_generation_prompt(
            curriculum_data=curriculum_data,
            user_prompt=enhanced_user_prompt,
            problem_count=problem_count,
            difficulty_distribution=difficulty_distribution
        )

        # ê·¸ë˜í”„ ë‹¨ì› í™•ì¸ ë¡œê·¸
        unit_name = curriculum_data.get('unit_name', '')
        if unit_name == "ê·¸ë˜í”„ì™€ ë¹„ë¡€":
            print(f"ğŸ“Š ê·¸ë˜í”„ì™€ ë¹„ë¡€ ë‹¨ì› ê°ì§€ - TikZ ìƒì„± í”„ë¡¬í”„íŠ¸ í™œì„±í™”")
            print(f"   ì±•í„°: {curriculum_data.get('chapter_name', '')}")

        # AI í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬ (target_count ì „ë‹¬)
        return self._call_ai_and_parse_response(prompt, target_count=problem_count)
    
    def _calculate_difficulty_distribution(self, problem_count: int, difficulty_ratio: Dict) -> str:
        """ë‚œì´ë„ ë¶„ë°° ê³„ì‚°"""
        if difficulty_ratio:
            # ë¹„ìœ¨ì— ë”°ë¥¸ ê° ë‚œì´ë„ë³„ ë¬¸ì œ ê°œìˆ˜ ê³„ì‚°
            total_problems = problem_count
            a_count = round(total_problems * difficulty_ratio['A'] / 100)
            b_count = round(total_problems * difficulty_ratio['B'] / 100)
            c_count = total_problems - a_count - b_count  # ë‚˜ë¨¸ì§€ëŠ” C
            
            return f"Aë‹¨ê³„ {a_count}ê°œ, Bë‹¨ê³„ {b_count}ê°œ, Cë‹¨ê³„ {c_count}ê°œ"
        else:
            return f"ëª¨ë“  ë¬¸ì œ Bë‹¨ê³„"
    
    def _call_ai_and_parse_response(self, prompt: str, max_retries: int = 3, target_count: int = None) -> List[Dict]:
        """AI í˜¸ì¶œ ë° ì‘ë‹µ íŒŒì‹± - ë¶€ë¶„ ì¬ìƒì„± ë¡œì§ í¬í•¨"""

        if target_count is None:
            # í”„ë¡¬í”„íŠ¸ì—ì„œ ë¬¸ì œ ê°œìˆ˜ ì¶”ì¶œ ì‹œë„ (ê¸°ë³¸ê°’ 1)
            target_count = 1

        valid_problems = []  # í•©ê²©í•œ ë¬¸ì œ ëˆ„ì 
        original_prompt = prompt  # ì›ë³¸ í”„ë¡¬í”„íŠ¸ ë°±ì—…

        for retry_attempt in range(max_retries):
            try:
                needed_count = target_count - len(valid_problems)

                if needed_count <= 0:
                    print(f"âœ… ëª©í‘œ ë‹¬ì„±: {len(valid_problems)}ê°œ ë¬¸ì œ ìƒì„± ì™„ë£Œ")
                    return valid_problems[:target_count]

                print(f"\n{'='*60}")
                print(f"ë¬¸ì œ ìƒì„± ì‹œë„ {retry_attempt + 1}/{max_retries}")
                print(f"í˜„ì¬ í•©ê²©: {len(valid_problems)}ê°œ / ëª©í‘œ: {target_count}ê°œ")
                print(f"ì¶”ê°€ í•„ìš”: {needed_count}ê°œ")
                print(f"{'='*60}\n")

                # ë¶€ì¡±í•œ ê°œìˆ˜ë§Œí¼ë§Œ ìƒì„±í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ì¡°ì •
                if len(valid_problems) > 0:
                    # ì´ë¯¸ ì¼ë¶€ í•©ê²©í•œ ê²½ìš° - ë¶€ì¡±í•œ ê°œìˆ˜ë§Œ ìš”ì²­
                    adjusted_prompt = self._adjust_prompt_for_needed_count(original_prompt, needed_count)
                else:
                    adjusted_prompt = prompt

                response = self.model.generate_content(adjusted_prompt)
                content = response.text

                # JSON ì¶”ì¶œ ë° íŒŒì‹±
                problems = self._extract_and_parse_json(content)

                # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
                validated_problems = []
                for problem in problems:
                    validated_problem = self._validate_basic_structure(problem)
                    validated_problems.append(validated_problem)

                # AI Judge ê²€ì¦
                print(f"ğŸ” AI Judge ê²€ì¦ ì‹œì‘ - {len(validated_problems)}ê°œ ë¬¸ì œ")

                invalid_problems = []

                current_batch_valid_count = 0
                for idx, problem in enumerate(validated_problems):
                    is_valid, scores, feedback = self._validate_with_ai_judge(problem)

                    # ìƒì„¸ ì ìˆ˜ ì¶œë ¥
                    score_detail = f"[ìˆ˜í•™ì •í™•ì„±:{scores.get('mathematical_accuracy', 0):.1f} " \
                                   f"ì •ë‹µì¼ì¹˜:{scores.get('consistency', 0):.1f} " \
                                   f"ì™„ê²°ì„±:{scores.get('completeness', 0):.1f} " \
                                   f"ë…¼ë¦¬ì„±:{scores.get('logic_flow', 0):.1f}]"

                    if is_valid:
                        current_batch_valid_count += 1
                        print(f"  âœ… ë¬¸ì œ {len(valid_problems) + current_batch_valid_count}ë²ˆ: VALID - í‰ê·  {scores['overall_score']:.1f}ì  {score_detail}")
                        valid_problems.append(problem)
                    else:
                        print(f"  âŒ ë¬¸ì œ {idx+1}ë²ˆ: INVALID - í‰ê·  {scores['overall_score']:.1f}ì  {score_detail}")
                        print(f"     ğŸ’¬ í”¼ë“œë°±: {feedback}")
                        invalid_problems.append({
                            "problem": problem,
                            "feedback": feedback,
                            "scores": scores
                        })

                # ëª©í‘œ ë‹¬ì„± í™•ì¸
                if len(valid_problems) >= target_count:
                    print(f"\nâœ… ëª©í‘œ ë‹¬ì„±: {len(valid_problems)}ê°œ ë¬¸ì œ ìƒì„± ì™„ë£Œ!")
                    return valid_problems[:target_count]

                # ì•„ì§ ë¶€ì¡±í•œ ê²½ìš°
                if retry_attempt < max_retries - 1:
                    shortage = target_count - len(valid_problems)
                    print(f"\nâš ï¸ ë¶€ì¡±: {shortage}ê°œ ì¶”ê°€ ìƒì„± í•„ìš” (í˜„ì¬ {len(valid_problems)}/{target_count})")

                    # í”¼ë“œë°±ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±
                    if invalid_problems:
                        prompt = self._rebuild_prompt_with_feedback(original_prompt, invalid_problems)
                else:
                    # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œë„ ë¶€ì¡±í•œ ê²½ìš°
                    shortage = target_count - len(valid_problems)
                    raise Exception(f"ê²€ì¦ ì‹¤íŒ¨: {max_retries}íšŒ ì‹œë„ í›„ {shortage}ê°œ ë¶€ì¡± (í˜„ì¬ {len(valid_problems)}/{target_count})")

            except json.JSONDecodeError as e:
                if retry_attempt < max_retries - 1:
                    print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({str(e)})")
                    continue
                else:
                    raise
            except Exception as e:
                if retry_attempt < max_retries - 1 and "ê²€ì¦ ì‹¤íŒ¨" not in str(e):
                    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ, ì¬ì‹œë„ ì¤‘... ({str(e)})")
                    continue
                else:
                    import traceback
                    error_msg = f"ë¬¸ì œ ìƒì„± ì˜¤ë¥˜: {str(e)}\n{traceback.format_exc()}"
                    print(error_msg)
                    raise Exception(error_msg)

        raise Exception(f"ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {max_retries}íšŒ ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨ (í˜„ì¬ {len(valid_problems)}/{target_count})")
    
    def _extract_and_parse_json(self, content: str) -> List[Dict]:
        """JSON ì¶”ì¶œ ë° íŒŒì‹± - ì™„ì „ ê°œì„  ë²„ì „"""
        # 1. JSON ë¸”ë¡ ì¶”ì¶œ
        json_str = self._extract_json_block(content)
        
        # 2. JSON ë¬¸ìì—´ ì „ì²˜ë¦¬
        preprocessed = self._preprocess_json_string(json_str)
        
        # 3. JSON íŒŒì‹± ì‹œë„
        try:
            result = json.loads(preprocessed)
            return result if isinstance(result, list) else [result]
        except json.JSONDecodeError as e:
            # 4. ê³ ê¸‰ ë³µêµ¬ ì‹œë„
            recovered = self._advanced_json_recovery(preprocessed)
            if recovered:
                return recovered
            
            # 5. ìµœí›„ì˜ ìˆ˜ë‹¨: ê°œë³„ ê°ì²´ íŒŒì‹±
            individual_problems = self._parse_individual_problems(preprocessed)
            if individual_problems:
                return individual_problems
            
            raise Exception(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}\nì›ë³¸: {json_str[:500]}...")
    
    def _extract_json_block(self, content: str) -> str:
        """JSON ë¸”ë¡ ì¶”ì¶œ"""
        # JSON ì½”ë“œ ë¸”ë¡ ì°¾ê¸°
        if "```json" in content:
            json_start = content.find("```json") + 7
            json_end = content.find("```", json_start)
            if json_end != -1:
                return content[json_start:json_end].strip()
        
        # ë°°ì—´ íŒ¨í„´ ì°¾ê¸°
        array_match = re.search(r'\[\s*\{.*?\}\s*\]', content, re.DOTALL)
        if array_match:
            return array_match.group(0)
        
        # ê·¸ëƒ¥ ì „ì²´ ë°˜í™˜
        return content.strip()
    
    def _preprocess_json_string(self, json_str: str) -> str:
        """JSON ë¬¸ìì—´ ì „ì²˜ë¦¬ - ì™„ì „ ê°œì„ """
        if not json_str:
            return "[]"
        
        # 1. ê¸°ë³¸ ì •ë¦¬
        cleaned = json_str.strip()
        
        # 2. LaTeX ìˆ˜ì‹ ë³´í˜¸ (ì„ì‹œ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ êµì²´)
        math_expressions = []
        
        def protect_math(match):
            expr = match.group(1)
            placeholder = f"__MATH_{len(math_expressions)}__"
            math_expressions.append(expr)
            return f'"{placeholder}"' if match.group(0).startswith('"') else placeholder
        
        # $ ... $ í˜•íƒœì˜ ìˆ˜ì‹ ë³´í˜¸
        cleaned = re.sub(r'\$([^$]+)\$', protect_math, cleaned)
        
        # 3. ì œì–´ ë¬¸ì ë° ì˜ëª»ëœ ë¬¸ì ì œê±°
        cleaned = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', cleaned)
        
        # 4. ë¬¸ìì—´ ë‚´ ì¤„ë°”ê¿ˆ ì²˜ë¦¬
        def fix_multiline_strings(match):
            content = match.group(1)
            # ë¬¸ìì—´ ë‚´ë¶€ì˜ ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€ê²½
            content = re.sub(r'\s*\n\s*', ' ', content)
            return f'"{content}"'
        
        # ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¬¸ìì—´ ë‚´ë¶€ ì •ë¦¬
        cleaned = re.sub(r'"([^"]*)"', fix_multiline_strings, cleaned)
        
        # 5. JSON êµ¬ì¡° ì •ë¦¬
        # ëë¶€ë¶„ ì‰¼í‘œ ì œê±°
        cleaned = re.sub(r',\s*}', '}', cleaned)
        cleaned = re.sub(r',\s*]', ']', cleaned)
        
        # ëˆ„ë½ëœ ì‰¼í‘œ ì¶”ê°€
        cleaned = re.sub(r'}\s*{', '},{', cleaned)
        cleaned = re.sub(r']\s*\[', '],[', cleaned)
        
        # 6. í•„ë“œëª… ì •ë¦¬ (ë”°ì˜´í‘œ í™•ì¸)
        field_names = ['question', 'choices', 'correct_answer', 'explanation', 
                      'problem_type', 'difficulty', 'has_diagram', 'diagram_type', 
                      'diagram_elements']
        
        for field in field_names:
            # ë”°ì˜´í‘œ ì—†ëŠ” í•„ë“œëª…ì— ë”°ì˜´í‘œ ì¶”ê°€
            cleaned = re.sub(f'(?<!")\\b{field}\\b(?!")', f'"{field}"', cleaned)
        
        # 7. ìˆ˜ì‹ ë³µì›
        for i, expr in enumerate(math_expressions):
            placeholder = f"__MATH_{i}__"
            # LaTeX ë°±ìŠ¬ë˜ì‹œ ì´ìŠ¤ì¼€ì´í”„
            escaped_expr = expr.replace('\\', '\\\\').replace('"', '\\"')
            cleaned = cleaned.replace(placeholder, f"${escaped_expr}$")
        
        # 8. ìµœì¢… ì •ë¦¬
        cleaned = re.sub(r'\s+', ' ', cleaned)  # ë‹¤ì¤‘ ê³µë°± ì œê±°
        
        return cleaned
    
    def _advanced_json_recovery(self, json_str: str) -> Optional[List[Dict]]:
        """ê³ ê¸‰ JSON ë³µêµ¬ ì‹œë„"""
        try:
            # 1. ë°°ì—´ êµ¬ì¡° í™•ì¸ ë° ìˆ˜ì •
            if not json_str.startswith('['):
                json_str = '[' + json_str
            if not json_str.endswith(']'):
                json_str = json_str + ']'
            
            # 2. ê°œë³„ ê°ì²´ ì¶”ì¶œ ì‹œë„
            object_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
            matches = re.findall(object_pattern, json_str)
            
            if matches:
                problems = []
                for match in matches:
                    try:
                        # ê°œë³„ ê°ì²´ íŒŒì‹±
                        obj = json.loads(match)
                        if self._is_valid_problem(obj):
                            problems.append(obj)
                    except:
                        continue
                
                if problems:
                    return problems
            
            # 3. êµ¬ì¡° ë³µêµ¬ ì‹œë„
            # ì˜ëª»ëœ ì¤‘ì²© êµ¬ì¡° ìˆ˜ì •
            json_str = re.sub(r'(\w+):\s*{', r'"\1": {', json_str)
            json_str = re.sub(r'(\w+):\s*\[', r'"\1": [', json_str)
            json_str = re.sub(r'(\w+):\s*"', r'"\1": "', json_str)
            json_str = re.sub(r'(\w+):\s*(\d+)', r'"\1": \2', json_str)
            json_str = re.sub(r'(\w+):\s*(true|false|null)', r'"\1": \2', json_str)
            
            return json.loads(json_str)
            
        except:
            return None
    
    def _parse_individual_problems(self, json_str: str) -> Optional[List[Dict]]:
        """ê°œë³„ ë¬¸ì œ ê°ì²´ íŒŒì‹± - ìµœí›„ì˜ ìˆ˜ë‹¨"""
        problems = []
        
        # ê° ë¬¸ì œ ê°ì²´ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì°¾ì•„ì„œ íŒŒì‹±
        current_pos = 0
        while True:
            # { ì°¾ê¸°
            start = json_str.find('{', current_pos)
            if start == -1:
                break
            
            # ë§¤ì¹­ë˜ëŠ” } ì°¾ê¸°
            bracket_count = 0
            end = start
            for i in range(start, len(json_str)):
                if json_str[i] == '{':
                    bracket_count += 1
                elif json_str[i] == '}':
                    bracket_count -= 1
                    if bracket_count == 0:
                        end = i + 1
                        break
            
            if end > start:
                obj_str = json_str[start:end]
                try:
                    # ê°œë³„ ê°ì²´ íŒŒì‹± ì‹œë„
                    obj = json.loads(obj_str)
                    if self._is_valid_problem(obj):
                        problems.append(obj)
                except:
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ íŒŒì‹± ì‹œë„
                    manual_obj = self._manual_parse_problem(obj_str)
                    if manual_obj:
                        problems.append(manual_obj)
                
                current_pos = end
            else:
                break
        
        return problems if problems else None
    
    def _manual_parse_problem(self, obj_str: str) -> Optional[Dict]:
        """ìˆ˜ë™ìœ¼ë¡œ ë¬¸ì œ ê°ì²´ íŒŒì‹±"""
        try:
            problem = {}
            
            # í•„ë“œ ì¶”ì¶œ íŒ¨í„´
            patterns = {
                'question': r'"question"\s*:\s*"([^"]*(?:\\"[^"]*)*)"',
                'correct_answer': r'"correct_answer"\s*:\s*"([^"]*(?:\\"[^"]*)*)"',
                'explanation': r'"explanation"\s*:\s*"([^"]*(?:\\"[^"]*)*)"',
                'problem_type': r'"problem_type"\s*:\s*"([^"]*)"',
                'difficulty': r'"difficulty"\s*:\s*"([ABC])"',
                'has_diagram': r'"has_diagram"\s*:\s*(true|false)',
            }
            
            for field, pattern in patterns.items():
                match = re.search(pattern, obj_str, re.IGNORECASE)
                if match:
                    value = match.group(1)
                    if field == 'has_diagram':
                        problem[field] = value.lower() == 'true'
                    else:
                        # ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ ë³µì›
                        value = value.replace('\\"', '"')
                        problem[field] = value
            
            # choices ë°°ì—´ ì¶”ì¶œ
            choices_match = re.search(r'"choices"\s*:\s*\[([^\]]*)\]', obj_str)
            if choices_match:
                choices_str = choices_match.group(1)
                choices = []
                for choice_match in re.finditer(r'"([^"]*(?:\\"[^"]*)*)"', choices_str):
                    choice = choice_match.group(1).replace('\\"', '"')
                    choices.append(choice)
                problem['choices'] = choices
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if 'question' in problem and 'correct_answer' in problem:
                # ê¸°ë³¸ê°’ ì„¤ì •
                problem.setdefault('problem_type', 'short_answer')
                problem.setdefault('difficulty', 'B')
                problem.setdefault('has_diagram', False)
                problem.setdefault('explanation', '')
                
                return problem
            
        except:
            pass
        
        return None
    
    def _is_valid_problem(self, obj: Dict) -> bool:
        """ë¬¸ì œ ê°ì²´ ìœ íš¨ì„± ê²€ì‚¬"""
        required_fields = ['question', 'correct_answer']
        return all(field in obj for field in required_fields)
    
    def _validate_basic_structure(self, problem: Dict) -> Dict:
        """ê¸°ë³¸ êµ¬ì¡° ê²€ì¦ë§Œ ìˆ˜í–‰ - LaTeXëŠ” Geminiê°€ ì™„ë²½í•˜ê²Œ ìƒì„±"""
        # 1. í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
        problem = self._ensure_required_fields(problem)

        # 2. ë°ì´í„° íƒ€ì… ê²€ì¦ë§Œ ìˆ˜í–‰
        problem = self._validate_data_types(problem)

        return problem
    
    def _ensure_required_fields(self, problem: Dict) -> Dict:
        """í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •"""
        defaults = {
            'question': '',
            'correct_answer': '',
            'explanation': '',
            'problem_type': 'short_answer',
            'difficulty': 'B',
            'has_diagram': False,
            'diagram_type': None,
            'diagram_elements': None
        }
        
        for field, default_value in defaults.items():
            if field not in problem:
                problem[field] = default_value
        
        # choices í•„ë“œëŠ” ê°ê´€ì‹ì¼ ë•Œë§Œ
        if problem['problem_type'] == 'multiple_choice' and 'choices' not in problem:
            problem['choices'] = []
        
        return problem
    
    
    def _validate_data_types(self, problem: Dict) -> Dict:
        """ê¸°ë³¸ ë°ì´í„° íƒ€ì… ê²€ì¦ë§Œ ìˆ˜í–‰"""
        # difficultyëŠ” ëŒ€ë¬¸ìë¡œ
        if 'difficulty' in problem:
            difficulty = str(problem['difficulty']).upper()
            if difficulty not in ['A', 'B', 'C']:
                problem['difficulty'] = 'B'
            else:
                problem['difficulty'] = difficulty

        # problem_type ê¸°ë³¸ ê²€ì¦
        valid_types = ['multiple_choice', 'short_answer', 'essay']
        if 'problem_type' in problem:
            if problem['problem_type'] not in valid_types:
                # ê°ê´€ì‹ ì—¬ë¶€ë¡œ ìë™ íŒë‹¨
                if 'choices' in problem and problem['choices']:
                    problem['problem_type'] = 'multiple_choice'
                else:
                    problem['problem_type'] = 'short_answer'

        # has_diagramì€ booleanìœ¼ë¡œ
        if 'has_diagram' in problem:
            if isinstance(problem['has_diagram'], str):
                problem['has_diagram'] = problem['has_diagram'].lower() == 'true'
            elif not isinstance(problem['has_diagram'], bool):
                problem['has_diagram'] = False

        return problem

    def _validate_with_ai_judge(self, problem: Dict) -> tuple:
        """
        AI Judgeë¡œ ë¬¸ì œ ê²€ì¦ (OpenAI GPT-4o-mini) - ì•ˆì „ í•„í„° ë¬¸ì œ í•´ê²°

        Returns:
            (is_valid: bool, scores: dict, feedback: str)
        """
        try:
            # ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš© (OpenAIëŠ” LaTeX ì²˜ë¦¬ ê°€ëŠ¥)
            question = problem.get('question', '')
            correct_answer = problem.get('correct_answer', '')
            explanation = problem.get('explanation', '')
            problem_type = problem.get('problem_type', '')
            choices = problem.get('choices', [])
            choices_text = ', '.join(map(str, choices)) if choices else 'None'

            # tikz_codeì™€ diagram ê´€ë ¨ í•„ë“œëŠ” ê²€ì¦ì—ì„œ ì œì™¸ (ì„ íƒì  í•„ë“œ)
            has_diagram = problem.get('has_diagram', False)
            diagram_note = " (Note: This problem may include graph/diagram fields which are optional and should not affect validation.)" if has_diagram else ""

            validation_prompt = f"""You are a math education expert. Please validate the following math problem.

The problem data is as follows:
- Question: {question}
- Correct Answer: {correct_answer}
- Explanation: {explanation}
- Problem Type: {problem_type}
- Choices: {choices_text}{diagram_note}

Evaluation criteria:
1. mathematical_accuracy (1-5): No mathematical or logical errors.
2. consistency (1-5): The final answer in the explanation matches the correct_answer.
3. completeness (1-5): All required fields are present (e.g., multiple_choice must have 4 choices). IGNORE optional fields like tikz_code, diagram_type, has_diagram.
4. logic_flow (1-5): The explanation is logical and easy to follow.

Return ONLY valid JSON (no markdown, no code blocks):
{{
  "scores": {{"mathematical_accuracy": <score>, "consistency": <score>, "completeness": <score>, "logic_flow": <score>}},
  "overall_score": <average>,
  "decision": "VALID" or "INVALID",
  "feedback": "<brief feedback>"
}}

Decision rule: `consistency` must be 4 or higher, AND the average of the other scores must be 3.5 or higher to be "VALID".
"""

            # OpenAI API í˜¸ì¶œ
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a math education expert who validates math problems and returns structured JSON responses."},
                    {"role": "user", "content": validation_prompt}
                ],
                temperature=0.1,
                max_tokens=500,
                response_format={"type": "json_object"}  # JSON ëª¨ë“œ ê°•ì œ
            )

            result_text = response.choices[0].message.content.strip()

            # JSON íŒŒì‹±
            result = json.loads(result_text)

            is_valid = result.get('decision') == 'VALID'
            scores = result.get('scores', {})
            scores['overall_score'] = result.get('overall_score', 0)
            feedback = result.get('feedback', 'No feedback')

            return is_valid, scores, feedback

        except json.JSONDecodeError as e:
            # JSON íŒŒì‹± ì˜¤ë¥˜ëŠ” ì¬ë°œìƒì‹œì¼œ ì¬ì‹œë„ ìœ ë„
            print(f"âŒ AI Judge ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            print(f"   ì‘ë‹µ ë‚´ìš©: {result_text[:200]}...")
            raise Exception(f"AI Judge validation failed - invalid JSON response: {str(e)}")

        except Exception as e:
            # ê·¸ ì™¸ ì˜¤ë¥˜ëŠ” ì¬ë°œìƒì‹œì¼œ ì¬ì‹œë„
            print(f"âŒ AI Judge ê²€ì¦ ì˜¤ë¥˜: {str(e)}")
            raise Exception(f"AI Judge validation error: {str(e)}")

    def _adjust_prompt_for_needed_count(self, original_prompt: str, needed_count: int) -> str:
        """ë¶€ì¡±í•œ ê°œìˆ˜ë§Œí¼ë§Œ ìƒì„±í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ì¡°ì •"""
        import re

        # ë¬¸ì œ ê°œìˆ˜ íŒ¨í„´ ì°¾ê¸° ë° êµì²´
        patterns = [
            (r'(\d+)ê°œì˜?\s*ë¬¸ì œ', f'{needed_count}ê°œ ë¬¸ì œ'),
            (r'(\d+)\s*problems?', f'{needed_count} problems'),
            (r'ì •í™•íˆ\s*(\d+)ê°œ', f'ì •í™•íˆ {needed_count}ê°œ'),
            (r'Total Problems to Generate\*\*:\s*(\d+)', f'Total Problems to Generate**: {needed_count}'),
            (r'create\s+(\d+)\s+perfectly', f'create {needed_count} perfectly'),
            (r'Ensure the total count is\s+(\d+)', f'Ensure the total count is {needed_count}')
        ]

        adjusted = original_prompt
        for pattern, replacement in patterns:
            adjusted = re.sub(pattern, replacement, adjusted, flags=re.IGNORECASE)

        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ì¡°ì •: {needed_count}ê°œ ìƒì„±í•˜ë„ë¡ ìˆ˜ì •")
        return adjusted

    def _rebuild_prompt_with_feedback(self, original_prompt: str, invalid_problems: List[Dict]) -> str:
        """í”¼ë“œë°±ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±"""

        feedback_text = "\n\n**IMPORTANT: Previous attempt had issues. Fix these:**\n"
        for idx, item in enumerate(invalid_problems):
            feedback_text += f"\nProblem {idx+1} feedback:\n"
            feedback_text += f"- Scores: mathematical_accuracy={item['scores'].get('mathematical_accuracy')}, "
            feedback_text += f"consistency={item['scores'].get('consistency')}, "
            feedback_text += f"completeness={item['scores'].get('completeness')}, "
            feedback_text += f"logic_flow={item['scores'].get('logic_flow')}\n"
            feedback_text += f"- Issue: {item['feedback']}\n"

        feedback_text += "\n**MUST ensure**: consistency >= 4 (explanation's answer = correct_answer), all scores >= 3.5\n"

        return original_prompt + feedback_text